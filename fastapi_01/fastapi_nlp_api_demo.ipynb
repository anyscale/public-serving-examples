{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastAPI NLP API Demo\n",
    "\n",
    "This notebook demonstrates how to interact with the NLP API endpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import websocket\n",
    "import threading\n",
    "from IPython.display import display, HTML, JSON, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API configuration\n",
    "BASE_URL = \"http://localhost:8000/api/v1\"\n",
    "# Default credentials (update these)\n",
    "USERNAME = \"admin\"\n",
    "PASSWORD = \"password\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Authentication\n",
    "\n",
    "First, let's authenticate with the API to get a token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(username, password):\n",
    "    \"\"\"Get authentication token from the API.\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/token\",\n",
    "        data={\"username\": username, \"password\": password},\n",
    "        headers={\"Content-Type\": \"application/x-www-form-urlencoded\"}\n",
    "    )\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        token_data = response.json()\n",
    "        return token_data[\"access_token\"]\n",
    "    else:\n",
    "        print(f\"Authentication failed with status code {response.status_code}\")\n",
    "        print(response.text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the authentication token\n",
    "token = get_token(USERNAME, PASSWORD)\n",
    "\n",
    "if token:\n",
    "    print(f\"Authentication successful! Token obtained.\")\n",
    "    # Set up the default headers with our token\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "else:\n",
    "    print(\"Failed to get token. Please check your credentials.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Health Check\n",
    "\n",
    "Let's check if the API is healthy and all models are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_health():\n",
    "    \"\"\"Check the health status of the API.\"\"\"\n",
    "    response = requests.get(f\"{BASE_URL}/health\")\n",
    "    return response.json()\n",
    "\n",
    "health_status = check_health()\n",
    "display(JSON(health_status))\n",
    "\n",
    "# Visual representation of model availability\n",
    "if \"models\" in health_status:\n",
    "    models_df = pd.DataFrame([{\"model\": k, \"available\": v} for k, v in health_status[\"models\"].items()])\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=\"model\", y=\"available\", data=models_df)\n",
    "    plt.title(\"Model Availability\")\n",
    "    plt.ylabel(\"Available\")\n",
    "    plt.xlabel(\"Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Get User Information\n",
    "\n",
    "Let's check the current user's information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_info():\n",
    "    \"\"\"Get current user information.\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"{BASE_URL}/users/me\",\n",
    "        headers=headers\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "user_info = get_user_info()\n",
    "display(JSON(user_info))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Sentiment Analysis\n",
    "\n",
    "Let's analyze the sentiment of some example texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text, language=\"en\"):\n",
    "    \"\"\"Analyze the sentiment of the given text.\"\"\"\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"language\": language\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/sentiment\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with some example texts\n",
    "example_texts = [\n",
    "    \"I love this product, it's amazing!\",\n",
    "    \"This is the worst experience ever.\",\n",
    "    \"The package arrived on time.\",\n",
    "    \"I'm not sure what to think about this.\"\n",
    "]\n",
    "\n",
    "sentiment_results = []\n",
    "for text in example_texts:\n",
    "    result = analyze_sentiment(text)\n",
    "    sentiment_results.append(result)\n",
    "    display(JSON(result))\n",
    "    \n",
    "# Create a visualization of the sentiment scores\n",
    "sentiment_df = pd.DataFrame([\n",
    "    {\"text\": r[\"text\"], \"sentiment\": r[\"sentiment\"], \"score\": r[\"score\"]} \n",
    "    for r in sentiment_results\n",
    "])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = sns.barplot(x=\"text\", y=\"score\", hue=\"sentiment\", data=sentiment_df)\n",
    "plt.title(\"Sentiment Analysis Results\")\n",
    "plt.ylabel(\"Confidence Score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Named Entity Recognition\n",
    "\n",
    "Let's extract entities from some sample texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(text):\n",
    "    \"\"\"Extract named entities from the given text.\"\"\"\n",
    "    payload = {\"text\": text}\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/entities\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_text = \"Apple is looking to buy U.K. startup for $1 billion in January 2023. Microsoft CEO Satya Nadella announced the news in New York.\"\n",
    "entities = extract_entities(ner_text)\n",
    "display(JSON(entities))\n",
    "\n",
    "# Create a visualization of entity types\n",
    "if entities and \"entities\" in entities:\n",
    "    entities_df = pd.DataFrame(entities[\"entities\"])\n",
    "    \n",
    "    # Count entities by type\n",
    "    entity_counts = entities_df[\"type\"].value_counts().reset_index()\n",
    "    entity_counts.columns = [\"entity_type\", \"count\"]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    sns.barplot(x=\"entity_type\", y=\"count\", data=entity_counts)\n",
    "    plt.title(\"Entity Types in Text\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"Entity Type\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the annotated text\n",
    "    def highlight_entities(text, entities):\n",
    "        \"\"\"Highlight entities in text with HTML colors.\"\"\"\n",
    "        colors = {\n",
    "            \"PERSON\": \"#FF9999\",\n",
    "            \"ORG\": \"#99FF99\",\n",
    "            \"LOC\": \"#9999FF\",\n",
    "            \"DATE\": \"#FFFF99\",\n",
    "            \"TIME\": \"#99FFFF\",\n",
    "            \"MONEY\": \"#FF99FF\",\n",
    "            \"PERCENT\": \"#FFCC99\",\n",
    "        }\n",
    "        \n",
    "        # Sort entities by start position (reversed to avoid index issues)\n",
    "        sorted_entities = sorted(entities, key=lambda x: x[\"start\"], reverse=True)\n",
    "        \n",
    "        result = text\n",
    "        for entity in sorted_entities:\n",
    "            start = entity[\"start\"]\n",
    "            end = entity[\"end\"]\n",
    "            entity_type = entity[\"type\"]\n",
    "            entity_text = text[start:end]\n",
    "            color = colors.get(entity_type, \"#CCCCCC\")\n",
    "            \n",
    "            highlight = f'<span style=\"background-color: {color}; padding: 2px; border-radius: 3px;\" title=\"{entity_type}\">{entity_text}</span>'\n",
    "            result = result[:start] + highlight + result[end:]\n",
    "            \n",
    "        return result\n",
    "    \n",
    "    highlighted_text = highlight_entities(entities[\"text\"], entities[\"entities\"])\n",
    "    display(HTML(f\"<p style='font-size: 16px; line-height: 1.5;'>{highlighted_text}</p>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Entity Filtering\n",
    "\n",
    "Let's try filtering entities by type and confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entities(text, entity_types=None, min_score=0.5):\n",
    "    \"\"\"Filter entities by type and minimum confidence score.\"\"\"\n",
    "    params = {\"text\": text, \"min_score\": min_score}\n",
    "    \n",
    "    if entity_types:\n",
    "        params[\"entity_types\"] = entity_types\n",
    "    \n",
    "    response = requests.get(\n",
    "        f\"{BASE_URL}/entities/filter\",\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter only for organizations and persons with high confidence\n",
    "filtered_entities = filter_entities(\n",
    "    ner_text, \n",
    "    entity_types=[\"ORG\", \"PERSON\"], \n",
    "    min_score=0.7\n",
    ")\n",
    "\n",
    "display(JSON(filtered_entities))\n",
    "\n",
    "# Compare filtered vs total entities count\n",
    "if \"filtered_from\" in filtered_entities and \"filtered_to\" in filtered_entities:\n",
    "    filter_stats = pd.DataFrame([\n",
    "        {\"category\": \"Total Entities\", \"count\": filtered_entities[\"filtered_from\"]},\n",
    "        {\"category\": \"Filtered Entities\", \"count\": filtered_entities[\"filtered_to\"]}\n",
    "    ])\n",
    "    \n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.barplot(x=\"category\", y=\"count\", data=filter_stats)\n",
    "    plt.title(\"Entity Filtering Results\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xlabel(\"\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Text Classification\n",
    "\n",
    "Let's classify some sample texts into categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_text(text, labels, multi_label=False):\n",
    "    \"\"\"Classify text into given categories.\"\"\"\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"labels\": labels,\n",
    "        \"multi_label\": multi_label\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/classify\",\n",
    "        headers=headers,\n",
    "        json=payload\n",
    "    )\n",
    "    \n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get classification presets\n",
    "def get_classification_preset(preset_name):\n",
    "    \"\"\"Get predefined label sets for classification.\"\"\"\n",
    "    response = requests.get(\n",
    "        f\"{BASE_URL}/classify/presets/{preset_name}\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Get the topics preset\n",
    "topics_preset = get_classification_preset(\"topics\")\n",
    "display(JSON(topics_preset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example texts for classification\n",
    "classification_examples = [\n",
    "    \"Climate change poses serious environmental challenges\",\n",
    "    \"The latest smartphone features a 108MP camera and advanced AI\",\n",
    "    \"The stock market plunged 500 points due to economic concerns\",\n",
    "    \"The team won the championship with a last-minute goal\"\n",
    "]\n",
    "\n",
    "# Use the topics from the preset\n",
    "topic_labels = topics_preset[\"labels\"]\n",
    "\n",
    "# Classify each example text\n",
    "classification_results = []\n",
    "for text in classification_examples:\n",
    "    result = classify_text(text, topic_labels)\n",
    "    classification_results.append(result)\n",
    "    display(JSON(result))\n",
    "    \n",
    "# Visualize top categories for each text\n",
    "classification_data = []\n",
    "for result in classification_results:\n",
    "    # Sort labels by score (highest first) and get top label\n",
    "    top_label = sorted(result[\"labels\"], key=lambda x: x.get(\"score\", 0), reverse=True)[0]\n",
    "    classification_data.append({\n",
    "        \"text\": result[\"text\"][:30] + \"...\",  # Truncate long texts\n",
    "        \"top_category\": top_label.get(\"label\"),\n",
    "        \"score\": top_label.get(\"score\")\n",
    "    })\n",
    "\n",
    "class_df = pd.DataFrame(classification_data)\n",
    "plt.figure(figsize=(12, 6))\n",
    "bars = sns.barplot(x=\"text\", y=\"score\", hue=\"top_category\", data=class_df)\n",
    "plt.title(\"Top Categories for Example Texts\")\n",
    "plt.ylabel(\"Confidence Score\")\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "plt.tight_layout()\n",
    "plt.legend(title=\"Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Custom Classification with Query Parameters\n",
    "\n",
    "Let's try the custom classification endpoint that uses query parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_classify(text, labels):\n",
    "    \"\"\"Classify text using query parameters.\"\"\"\n",
    "    params = {\n",
    "        \"text\": text,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "    \n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/classify/custom\",\n",
    "        headers=headers,\n",
    "        params=params\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "# Try with intent classification\n",
    "intent_result = custom_classify(\n",
    "    \"How do I reset my password?\", \n",
    "    [\"question\", \"complaint\", \"feedback\", \"request\"]\n",
    ")\n",
    "display(JSON(intent_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "## 8. Streaming Text Analysis\n",
    "\n",
    "Now let's test the streaming analysis capabilities of the API.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_stream_analyze_text():\n",
    "    \"\"\"Demonstrate streaming text analysis using Server-Sent Events.\"\"\"\n",
    "    # Example long text for analysis\n",
    "    text = \"\"\"The Apple conference in San Francisco was attended by Tim Cook, who announced new products.\n",
    "    Microsoft and Google were also represented at the event.\n",
    "    The new iPhone impressed many analysts with its innovative features.\n",
    "    The event took place at the Moscone Center in downtown San Francisco.\n",
    "    Investors reacted positively to the announcements, with Apple stock rising 3% after the conference.\"\"\"\n",
    "    \n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"analysis_types\": [\"sentiment\", \"entities\"],\n",
    "        \"chunk_delay\": 0.5  # Add artificial delay for demonstration purposes\n",
    "    }\n",
    "    \n",
    "    print(\"Starting streaming analysis...\")\n",
    "    print(f\"Text to analyze: {text[:100]}...\")\n",
    "    \n",
    "    # Make the request with stream=True to handle the SSE stream\n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/streaming/analyze\",\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return\n",
    "    \n",
    "    # Process the SSE stream\n",
    "    print(\"\\nReceiving streaming results:\\n\")\n",
    "    line_buffer = \"\"\n",
    "    \n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode('utf-8')\n",
    "            \n",
    "            # SSE lines start with \"data: \"\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                data_json = line_str[6:]  # Remove \"data: \" prefix\n",
    "                try:\n",
    "                    data = json.loads(data_json)\n",
    "                    \n",
    "                    # Handle different types of messages\n",
    "                    if \"status\" in data:\n",
    "                        # Status message\n",
    "                        print(f\"Status: {data['status']} - {data.get('message', '')}\")\n",
    "                    else:\n",
    "                        # Chunk result\n",
    "                        print(f\"\\nChunk {data['chunk_id'] + 1}/{data['total_chunks']} \" +\n",
    "                              f\"({data['progress'] * 100:.1f}%):\")\n",
    "                        print(f\"Text: {data['chunk_text']}\")\n",
    "                        \n",
    "                        if \"sentiment\" in data:\n",
    "                            print(f\"Sentiment: {data['sentiment']} \" +\n",
    "                                  f\"(Score: {data['sentiment_score']:.4f})\")\n",
    "                        \n",
    "                        if \"entities\" in data and data[\"entities\"]:\n",
    "                            print(\"Entities:\")\n",
    "                            for entity in data[\"entities\"]:\n",
    "                                print(f\"  - {entity['word']} ({entity['entity']})\")\n",
    "                        \n",
    "                        print(f\"Processing time: {data['processing_time']:.4f} seconds\")\n",
    "                \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error parsing JSON: {data_json}\")\n",
    "    \n",
    "    print(\"\\nStreaming completed.\")\n",
    "\n",
    "# Run the streaming test\n",
    "test_stream_analyze_text()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Interactive Streaming Demo with Visualization\n",
    "\n",
    "Let's create a more advanced demo with visualization to better demonstrate\n",
    "the streaming capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_streaming_analysis():\n",
    "    \"\"\"Create a visualization of streaming analysis results.\"\"\"\n",
    "    # Set up a larger example text with mixed sentiments\n",
    "    text = \"\"\"The new iPhone launch was a tremendous success with record-breaking sales.\n",
    "    However, some users complained about battery life issues.\n",
    "    Apple's CEO announced that software updates would address these concerns.\n",
    "    Critics were skeptical that software alone could fix hardware limitations.\n",
    "    Nevertheless, investors remained confident in the company's long-term strategy.\n",
    "    The company also unveiled their latest MacBook Pro with significant performance improvements.\n",
    "    Tech reviewers praised the new design and speed enhancements of the MacBook.\n",
    "    Meanwhile, competitors scrambled to announce their own product launches.\n",
    "    Overall market reaction was mixed as consumers evaluated the different options available.\"\"\"\n",
    "    \n",
    "    # Prepare the payload\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"analysis_types\": [\"sentiment\", \"entities\"],\n",
    "        \"chunk_delay\": 0.3  # Add artificial delay for demonstration purposes\n",
    "    }\n",
    "    \n",
    "    # Set up visualization\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    # Tracking data\n",
    "    sentiment_scores = []\n",
    "    chunk_texts = []\n",
    "    entities_count = []\n",
    "    processing_times = []\n",
    "    progress_values = []\n",
    "    \n",
    "    print(\"Starting interactive streaming analysis visualization...\\n\")\n",
    "    \n",
    "    # Make the request with stream=True to handle the SSE stream\n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/streaming/analyze\",\n",
    "        headers=headers,\n",
    "        json=payload,\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"Error: {response.status_code} - {response.text}\")\n",
    "        return\n",
    "    \n",
    "    # Process the SSE stream\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode('utf-8')\n",
    "            \n",
    "            # SSE lines start with \"data: \"\n",
    "            if line_str.startswith(\"data: \"):\n",
    "                data_json = line_str[6:]  # Remove \"data: \" prefix\n",
    "                try:\n",
    "                    data = json.loads(data_json)\n",
    "                    \n",
    "                    # Status message\n",
    "                    if \"status\" in data:\n",
    "                        print(f\"Status: {data['status']} - {data.get('message', '')}\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Chunk result - update visualization data\n",
    "                    chunk_id = data['chunk_id']\n",
    "                    chunk_text = data['chunk_text']\n",
    "                    \n",
    "                    # Get data from chunk\n",
    "                    sentiment_score = data.get('sentiment_score', 0.5)\n",
    "                    entity_count = len(data.get('entities', []))\n",
    "                    processing_time = data.get('processing_time', 0)\n",
    "                    progress = data.get('progress', 0)\n",
    "                    \n",
    "                    # Store data for visualization\n",
    "                    sentiment_scores.append(sentiment_score)\n",
    "                    chunk_texts.append(f\"Chunk {chunk_id+1}\")\n",
    "                    entities_count.append(entity_count)\n",
    "                    processing_times.append(processing_time)\n",
    "                    progress_values.append(progress)\n",
    "                    \n",
    "                    # Update visualization\n",
    "                    clear_output(wait=True)\n",
    "                    \n",
    "                    # Create a figure with 3 subplots\n",
    "                    fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(14, 12))\n",
    "                    \n",
    "                    # Plot 1: Sentiment scores by chunk\n",
    "                    sentiment_colors = ['red' if s < 0.4 else 'green' if s > 0.6 else 'gray' \n",
    "                                       for s in sentiment_scores]\n",
    "                    ax1.bar(chunk_texts, sentiment_scores, color=sentiment_colors)\n",
    "                    ax1.set_ylim(0, 1)\n",
    "                    ax1.set_title('Sentiment Scores by Chunk (>0.6: Positive, <0.4: Negative)')\n",
    "                    ax1.set_ylabel('Sentiment Score')\n",
    "                    ax1.axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "                    \n",
    "                    # Plot 2: Entity count by chunk\n",
    "                    ax2.bar(chunk_texts, entities_count, color='skyblue')\n",
    "                    ax2.set_title('Named Entities Found by Chunk')\n",
    "                    ax2.set_ylabel('Entity Count')\n",
    "                    \n",
    "                    # Plot 3: Processing time by chunk\n",
    "                    ax3.plot(chunk_texts, processing_times, marker='o', color='purple')\n",
    "                    ax3.set_title('Processing Time by Chunk')\n",
    "                    ax3.set_ylabel('Time (seconds)')\n",
    "                    ax3.set_ylim(0, max(processing_times) * 1.2 if processing_times else 1)\n",
    "                    \n",
    "                    plt.tight_layout()\n",
    "                    plt.show()\n",
    "                    \n",
    "                    # Print current chunk info\n",
    "                    print(f\"\\nProcessing chunk {chunk_id + 1}/{data['total_chunks']} \" +\n",
    "                          f\"({progress * 100:.1f}% complete):\")\n",
    "                    print(f\"Text: {chunk_text}\")\n",
    "                    \n",
    "                    if \"sentiment\" in data:\n",
    "                        sentiment = data['sentiment']\n",
    "                        print(f\"Sentiment: {sentiment} (Score: {sentiment_score:.4f})\")\n",
    "                    \n",
    "                    if \"entities\" in data and data[\"entities\"]:\n",
    "                        print(\"Entities:\")\n",
    "                        for entity in data[\"entities\"]:\n",
    "                            print(f\"  - {entity['word']} ({entity['entity']})\")\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Error parsing JSON: {data_json}\")\n",
    "    \n",
    "    # Final visualization with all data\n",
    "    plt.figure(figsize=(16, 12))\n",
    "    \n",
    "    # Create summary dataframe\n",
    "    summary_df = pd.DataFrame({\n",
    "        'Chunk': chunk_texts,\n",
    "        'Sentiment Score': sentiment_scores,\n",
    "        'Entity Count': entities_count,\n",
    "        'Processing Time': processing_times\n",
    "    })\n",
    "    \n",
    "    print(\"\\nStreaming Analysis Summary:\")\n",
    "    display(summary_df)\n",
    "    \n",
    "    # Final visualization with all metrics together\n",
    "    fig, ax = plt.subplots(figsize=(14, 6))\n",
    "    \n",
    "    # Plot sentiment scores and entities count on the same graph with different scales\n",
    "    ax.set_xlabel('Chunk')\n",
    "    ax.set_ylabel('Sentiment Score', color='green')\n",
    "    line1 = ax.plot(chunk_texts, sentiment_scores, 'g-', marker='o', label='Sentiment Score')\n",
    "    ax.tick_params(axis='y', labelcolor='green')\n",
    "    ax.set_ylim(0, 1)\n",
    "    \n",
    "    ax2 = ax.twinx()\n",
    "    ax2.set_ylabel('Entity Count', color='blue')\n",
    "    line2 = ax2.plot(chunk_texts, entities_count, 'b-', marker='s', label='Entity Count')\n",
    "    ax2.tick_params(axis='y', labelcolor='blue')\n",
    "    \n",
    "    # Add a third y-axis for processing time\n",
    "    ax3 = ax.twinx()\n",
    "    ax3.spines[\"right\"].set_position((\"axes\", 1.1))\n",
    "    ax3.set_ylabel('Processing Time (s)', color='red')\n",
    "    line3 = ax3.plot(chunk_texts, processing_times, 'r-', marker='^', label='Processing Time')\n",
    "    ax3.tick_params(axis='y', labelcolor='red')\n",
    "    \n",
    "    # Add legends\n",
    "    lines = line1 + line2 + line3\n",
    "    labels = [l.get_label() for l in lines]\n",
    "    ax.legend(lines, labels, loc='upper left')\n",
    "    \n",
    "    plt.title('Streaming Analysis Metrics by Chunk')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nStreaming analysis visualization completed.\")\n",
    "\n",
    "# Run the interactive visualization\n",
    "visualize_streaming_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Compare Streaming vs. Non-Streaming Analysis\n",
    "\n",
    "Let's compare the streaming analysis with the traditional non-streaming approach for the same text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_streaming_vs_regular():\n",
    "    \"\"\"Compare streaming vs. regular analysis for a long text.\"\"\"\n",
    "    # Example long text\n",
    "    text = \"\"\"The artificial intelligence conference in Silicon Valley attracted experts from Google, Microsoft, and OpenAI.\n",
    "    Researchers presented new breakthroughs in natural language processing and computer vision.\n",
    "    The event was hosted at Stanford University's AI Research Center last month.\n",
    "    Many discussions focused on ethical implications of advanced AI systems and their potential impact on society.\n",
    "    Several startups demonstrated innovative applications using transformer-based models for various industries.\n",
    "    Venture capitalists expressed strong interest in funding new AI initiatives, especially those focused on healthcare.\n",
    "    The conference concluded with a panel featuring Professors from MIT and Berkeley discussing future research directions.\"\"\"\n",
    "    \n",
    "    print(\"Comparing streaming vs. non-streaming analysis for the same text...\\n\")\n",
    "    print(f\"Text length: {len(text)} characters\\n\")\n",
    "    \n",
    "    # 1. Regular (non-streaming) approach\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Make request to the document endpoint (non-streaming)\n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/streaming/analyze/document\",\n",
    "        headers=headers,\n",
    "        json={\"text\": text, \"analysis_types\": [\"sentiment\", \"entities\"]}\n",
    "    )\n",
    "    \n",
    "    regular_result = response.json()\n",
    "    regular_time = time.time() - start_time\n",
    "    \n",
    "    print(f\"Regular analysis completed in {regular_time:.2f} seconds\")\n",
    "    \n",
    "    # 2. Streaming approach (measuring time to first result and total time)\n",
    "    start_time = time.time()\n",
    "    first_result_time = None\n",
    "    all_chunks = []\n",
    "    \n",
    "    # Make streaming request\n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/streaming/analyze\",\n",
    "        headers=headers,\n",
    "        json={\"text\": text, \"analysis_types\": [\"sentiment\", \"entities\"]},\n",
    "        stream=True\n",
    "    )\n",
    "    \n",
    "    # Process the SSE stream\n",
    "    for line in response.iter_lines():\n",
    "        if line:\n",
    "            line_str = line.decode('utf-8')\n",
    "            \n",
    "            if line_str.startswith(\"data: \"):\n",
    "                data_json = line_str[6:]\n",
    "                try:\n",
    "                    data = json.loads(data_json)\n",
    "                    \n",
    "                    # Skip status messages\n",
    "                    if \"status\" in data:\n",
    "                        continue\n",
    "                    \n",
    "                    # Record time to first result if not already set\n",
    "                    if first_result_time is None:\n",
    "                        first_result_time = time.time() - start_time\n",
    "                    \n",
    "                    # Store chunk data\n",
    "                    all_chunks.append(data)\n",
    "                    \n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "    \n",
    "    streaming_total_time = time.time() - start_time\n",
    "    \n",
    "    # Show results comparison\n",
    "    print(f\"Streaming analysis:\")\n",
    "    print(f\"  - Time to first result: {first_result_time:.2f} seconds\")\n",
    "    print(f\"  - Total time: {streaming_total_time:.2f} seconds\")\n",
    "    print(f\"  - Number of chunks: {len(all_chunks)}\")\n",
    "    \n",
    "    # Create comparison chart\n",
    "    labels = ['Regular Analysis', 'Streaming (First Result)', 'Streaming (Total)']\n",
    "    times = [regular_time, first_result_time, streaming_total_time]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(labels, times, color=['blue', 'green', 'orange'])\n",
    "    plt.title('Time Comparison: Regular vs. Streaming Analysis')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    \n",
    "    # Add time labels on top of bars\n",
    "    for bar, time_val in zip(bars, times):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.05, \n",
    "                f'{time_val:.2f}s', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare entity extraction\n",
    "    stream_entities = []\n",
    "    for chunk in all_chunks:\n",
    "        if 'entities' in chunk:\n",
    "            stream_entities.extend(chunk['entities'])\n",
    "    \n",
    "    regular_entities = []\n",
    "    if 'entities' in regular_result and 'all_entities' in regular_result['entities']:\n",
    "        regular_entities = regular_result['entities']['all_entities']\n",
    "    \n",
    "    print(f\"\\nEntity extraction comparison:\")\n",
    "    print(f\"  - Regular analysis: {len(regular_entities)} entities found\")\n",
    "    print(f\"  - Streaming analysis: {len(stream_entities)} entities found\")\n",
    "    \n",
    "    # Visualize entity types\n",
    "    if regular_entities:\n",
    "        # Count entity types\n",
    "        regular_entity_types = {}\n",
    "        for entity in regular_entities:\n",
    "            entity_type = entity['entity']\n",
    "            regular_entity_types[entity_type] = regular_entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "        stream_entity_types = {}\n",
    "        for entity in stream_entities:\n",
    "            entity_type = entity['entity']\n",
    "            stream_entity_types[entity_type] = stream_entity_types.get(entity_type, 0) + 1\n",
    "        \n",
    "        # Combine and visualize\n",
    "        all_types = set(list(regular_entity_types.keys()) + list(stream_entity_types.keys()))\n",
    "        \n",
    "        comparison_data = {\n",
    "            'Entity Type': [],\n",
    "            'Count': [],\n",
    "            'Method': []\n",
    "        }\n",
    "        \n",
    "        for entity_type in all_types:\n",
    "            comparison_data['Entity Type'].append(entity_type)\n",
    "            comparison_data['Count'].append(regular_entity_types.get(entity_type, 0))\n",
    "            comparison_data['Method'].append('Regular')\n",
    "            \n",
    "            comparison_data['Entity Type'].append(entity_type)\n",
    "            comparison_data['Count'].append(stream_entity_types.get(entity_type, 0))\n",
    "            comparison_data['Method'].append('Streaming')\n",
    "        \n",
    "        df = pd.DataFrame(comparison_data)\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x='Entity Type', y='Count', hue='Method', data=df)\n",
    "        plt.title('Entity Type Comparison: Regular vs. Streaming Analysis')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.legend(title='Analysis Method')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    print(\"\\nAnalysis comparison completed.\")\n",
    "\n",
    "# Run the comparison\n",
    "compare_streaming_vs_regular() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. WebSocket Connection\n",
    "\n",
    "Let's demonstrate the WebSocket API for real-time NLP processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPWebSocketClient:\n",
    "    def __init__(self, url=\"ws://localhost:8000/api/v1/ws\"):\n",
    "        self.url = url\n",
    "        self.ws = None\n",
    "        self.messages = []\n",
    "        self.connected = False\n",
    "    \n",
    "    def on_message(self, ws, message):\n",
    "        \"\"\"Callback when a message is received.\"\"\"\n",
    "        data = json.loads(message)\n",
    "        self.messages.append(data)\n",
    "        print(f\"Received: {json.dumps(data, indent=2)}\")\n",
    "    \n",
    "    def on_error(self, ws, error):\n",
    "        \"\"\"Callback for errors.\"\"\"\n",
    "        print(f\"Error: {error}\")\n",
    "    \n",
    "    def on_close(self, ws, close_status_code, close_msg):\n",
    "        \"\"\"Callback when connection is closed.\"\"\"\n",
    "        print(\"Connection closed\")\n",
    "        self.connected = False\n",
    "    \n",
    "    def on_open(self, ws):\n",
    "        \"\"\"Callback when connection is established.\"\"\"\n",
    "        print(\"Connection opened\")\n",
    "        self.connected = True\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Connect to the WebSocket.\"\"\"\n",
    "        self.ws = websocket.WebSocketApp(\n",
    "            self.url,\n",
    "            on_open=self.on_open,\n",
    "            on_message=self.on_message,\n",
    "            on_error=self.on_error,\n",
    "            on_close=self.on_close\n",
    "        )\n",
    "        \n",
    "        # Start WebSocket in a background thread\n",
    "        self.thread = threading.Thread(target=self.ws.run_forever)\n",
    "        self.thread.daemon = True\n",
    "        self.thread.start()\n",
    "        \n",
    "        # Wait for connection\n",
    "        time.sleep(1)\n",
    "        return self.connected\n",
    "    \n",
    "    def send(self, data):\n",
    "        \"\"\"Send data to the WebSocket.\"\"\"\n",
    "        if not self.ws or not self.connected:\n",
    "            print(\"Not connected\")\n",
    "            return False\n",
    "        \n",
    "        self.ws.send(json.dumps(data))\n",
    "        return True\n",
    "    \n",
    "    def disconnect(self):\n",
    "        \"\"\"Disconnect from the WebSocket.\"\"\"\n",
    "        if self.ws:\n",
    "            self.ws.close()\n",
    "        \n",
    "        # Wait for thread to finish\n",
    "        if hasattr(self, 'thread') and self.thread.is_alive():\n",
    "            self.thread.join(timeout=1)\n",
    "            \n",
    "        self.connected = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and connect the WebSocket client\n",
    "ws_client = NLPWebSocketClient()\n",
    "connected = ws_client.connect()\n",
    "\n",
    "if connected:\n",
    "    # Try sentiment analysis\n",
    "    ws_client.send({\"action\": \"sentiment\", \"text\": \"I'm really enjoying this API!\"})\n",
    "    time.sleep(1)  # Wait for response\n",
    "    \n",
    "    # Try entity recognition\n",
    "    ws_client.send({\"action\": \"entities\", \"text\": \"Google's headquarters are in Mountain View, California.\"})\n",
    "    time.sleep(1)  # Wait for response\n",
    "    \n",
    "    # Try classification\n",
    "    ws_client.send({\n",
    "        \"action\": \"classify\", \n",
    "        \"text\": \"This computer keeps crashing whenever I open the browser\", \n",
    "        \"labels\": [\"technical issue\", \"feedback\", \"question\"]\n",
    "    })\n",
    "    time.sleep(1)  # Wait for response\n",
    "    \n",
    "    # Wait a bit to receive all responses then disconnect\n",
    "    time.sleep(2)\n",
    "    ws_client.disconnect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Admin Actions\n",
    "\n",
    "Let's try using the admin-only endpoint to reload models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_models():\n",
    "    \"\"\"Reload NLP models (admin only).\"\"\"\n",
    "    response = requests.post(\n",
    "        f\"{BASE_URL}/admin/reload-models\",\n",
    "        headers=headers\n",
    "    )\n",
    "    \n",
    "    return response.json()\n",
    "\n",
    "reload_result = reload_models()\n",
    "display(JSON(reload_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrates how to interact with all the major endpoints of the NLP API. You can use this as a starting point to build applications that leverage these natural language processing capabilities.\n",
    "\n",
    "Here's a summary of what we've explored:\n",
    "\n",
    "1. Authentication and getting tokens\n",
    "2. API health checks\n",
    "3. Sentiment analysis\n",
    "4. Named entity recognition\n",
    "5. Text classification\n",
    "6. WebSocket connections for real-time processing\n",
    "7. Admin actions for system management\n",
    "\n",
    "These building blocks can be combined to create powerful NLP-based applications."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-pipeline-46rPltzW-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
