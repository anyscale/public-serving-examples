{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Ray Serve NLP Deployments\n",
    "\n",
    "This notebook demonstrates how to deploy and test the three NLP models using Ray Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "from ray import serve\n",
    "import time\n",
    "import asyncio\n",
    "import json\n",
    "from pprint import pprint\n",
    "\n",
    "# Import the deployment classes\n",
    "from example_app.serve.deployments.sentiment import SentimentAnalyzer\n",
    "from example_app.serve.deployments.classification import TextClassifier\n",
    "from example_app.serve.deployments.entities import EntityRecognizer\n",
    "\n",
    "# Import config\n",
    "from example_app.config import RAY_ADDRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray and Ray Serve\n",
    "\n",
    "First, we need to initialize Ray and start Ray Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray (if not already initialized)\n",
    "if not ray.is_initialized():\n",
    "    ray.init(address=RAY_ADDRESS, namespace=\"nlp_pipeline\")\n",
    "    print(\"Ray initialized\")\n",
    "else:\n",
    "    print(\"Ray already initialized\")\n",
    "    \n",
    "# Start Ray Serve\n",
    "serve.start(detached=True)\n",
    "print(\"Ray Serve started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Models\n",
    "\n",
    "Now we'll deploy each of the three NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy SentimentAnalyzer\n",
    "serve.run(SentimentAnalyzer.options(name=\"sentiment_analyzer\").bind(), name=\"sentiment_analyzer\", route_prefix=\"/sentiment\")\n",
    "print(\"SentimentAnalyzer deployed\")\n",
    "\n",
    "# Deploy TextClassifier\n",
    "serve.run(TextClassifier.options(name=\"text_classifier\").bind(), name=\"text_classifier\", route_prefix=\"/text_classifier\")\n",
    "print(\"TextClassifier deployed\")\n",
    "\n",
    "# Deploy EntityRecognizer\n",
    "serve.run(EntityRecognizer.options(name=\"entity_recognizer\").bind(), name=\"entity_recognizer\", route_prefix=\"/entity_recognizer\")\n",
    "print(\"EntityRecognizer deployed\")\n",
    "\n",
    "# Wait a moment for deployments to initialize\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Deployment Handles\n",
    "\n",
    "Next, we'll get handles to each deployment to make remote calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.serve import get_deployment_handle\n",
    "\n",
    "# Get handles to the deployments\n",
    "sentiment_analyzer = get_deployment_handle(\"sentiment_analyzer\", app_name=\"sentiment_analyzer\")\n",
    "text_classifier = get_deployment_handle(\"text_classifier\", app_name=\"text_classifier\")\n",
    "entity_recognizer = get_deployment_handle(\"entity_recognizer\", app_name=\"entity_recognizer\")\n",
    "\n",
    "print(\"Got handles to all deployments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SentimentAnalyzer\n",
    "\n",
    "Let's test the sentiment analyzer with some example sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_sentiment():\n",
    "    sample_texts = [\n",
    "        \"I really enjoyed this movie, it was fantastic!\",\n",
    "        \"The service was terrible and the food was cold.\",\n",
    "        \"This product is okay, not great but not bad either.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Testing Sentiment Analysis ===\\n\")\n",
    "    for text in sample_texts:\n",
    "        result = await sentiment_analyzer.analyze.remote(text)\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Sentiment: {result['sentiment']} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Processing time: {result['processing_time']:.4f} seconds\\n\")\n",
    "\n",
    "await test_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TextClassifier\n",
    "\n",
    "Now let's test the zero-shot text classifier with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_classification():\n",
    "    sample_texts = [\n",
    "        \"The iPhone 13 Pro Max has an amazing camera system and excellent battery life.\",\n",
    "        \"Scientists have discovered a new species of deep-sea fish that can glow in the dark.\",\n",
    "        \"The company reported a 15% increase in quarterly revenue, exceeding analyst expectations.\"\n",
    "    ]\n",
    "    \n",
    "    sample_labels = [\n",
    "        [\"technology\", \"sports\", \"politics\", \"entertainment\"],\n",
    "        [\"science\", \"arts\", \"business\", \"health\"],\n",
    "        [\"finance\", \"technology\", \"politics\", \"environment\"]\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Testing Text Classification ===\\n\")\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        labels = sample_labels[i]\n",
    "        result = await text_classifier.classify.remote(text, labels)\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Labels: {labels}\")\n",
    "        print(\"Results:\")\n",
    "        for label_info in result[\"labels\"]:\n",
    "            print(f\"  - {label_info['label']}: {label_info['score']:.4f}\")\n",
    "        print(f\"Processing time: {result['processing_time']:.4f} seconds\\n\")\n",
    "\n",
    "await test_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test EntityRecognizer\n",
    "\n",
    "Finally, let's test the named entity recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_entity_recognition():\n",
    "    sample_texts = [\n",
    "        \"Apple CEO Tim Cook announced the new iPhone at their headquarters in Cupertino, California yesterday.\",\n",
    "        \"The European Union and the United States are working on new regulations for artificial intelligence.\",\n",
    "        \"Microsoft's CEO Satya Nadella will be speaking at the conference in New York on Friday.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Testing Entity Recognition ===\\n\")\n",
    "    for text in sample_texts:\n",
    "        result = await entity_recognizer.recognize_entities.remote(text)\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"Entities:\")\n",
    "        \n",
    "        # Group entities by type\n",
    "        entities_by_type = {}\n",
    "        for entity in result[\"entities\"]:\n",
    "            entity_type = entity[\"type\"]\n",
    "            if entity_type not in entities_by_type:\n",
    "                entities_by_type[entity_type] = []\n",
    "            entities_by_type[entity_type].append(entity)\n",
    "        \n",
    "        # Print entities by type\n",
    "        for entity_type, entities in entities_by_type.items():\n",
    "            print(f\"  {entity_type}:\")\n",
    "            for entity in entities:\n",
    "                print(f\"    - {entity['text']} (Score: {entity['score']:.4f})\")\n",
    "                \n",
    "        print(f\"Processing time: {result['processing_time']:.4f} seconds\\n\")\n",
    "\n",
    "await test_entity_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Let's run a simple benchmark to measure the throughput of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def benchmark_models(num_requests=10):\n",
    "    print(\"\\n=== Performance Benchmarking ===\\n\")\n",
    "    \n",
    "    # Test data\n",
    "    sentiment_text = \"This product exceeded my expectations in every way possible!\"\n",
    "    classification_text = \"The president announced new economic policies yesterday.\"\n",
    "    classification_labels = [\"politics\", \"economics\", \"sports\", \"technology\"]\n",
    "    entity_text = \"Google and Microsoft are competing in the cloud services market in Europe.\"\n",
    "    \n",
    "    # Benchmark SentimentAnalyzer\n",
    "    print(f\"Benchmarking SentimentAnalyzer with {num_requests} requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tasks = [sentiment_analyzer.analyze.remote(sentiment_text) for _ in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average time per request: {total_time/num_requests:.4f} seconds\")\n",
    "    print(f\"Throughput: {num_requests/total_time:.2f} requests/second\\n\")\n",
    "    \n",
    "    # Benchmark TextClassifier\n",
    "    print(f\"Benchmarking TextClassifier with {num_requests} requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tasks = [text_classifier.classify.remote(classification_text, classification_labels) for _ in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average time per request: {total_time/num_requests:.4f} seconds\")\n",
    "    print(f\"Throughput: {num_requests/total_time:.2f} requests/second\\n\")\n",
    "    \n",
    "    # Benchmark EntityRecognizer\n",
    "    print(f\"Benchmarking EntityRecognizer with {num_requests} requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tasks = [entity_recognizer.recognize_entities.remote(entity_text) for _ in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average time per request: {total_time/num_requests:.4f} seconds\")\n",
    "    print(f\"Throughput: {num_requests/total_time:.2f} requests/second\\n\")\n",
    "\n",
    "# Run benchmark with 10 requests per model\n",
    "await benchmark_models(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally, we can shut down Ray Serve when we're done testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to shut down Ray Serve\n",
    "# serve.shutdown()\n",
    "# print(\"Ray Serve shut down\")\n",
    "\n",
    "print(\"All tests completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-pipeline-46rPltzW-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
