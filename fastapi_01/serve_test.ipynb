{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Ray Serve NLP Deployments\n",
    "\n",
    "This notebook demonstrates how to deploy and test the three NLP models using Ray Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import ray\n",
    "from ray import serve\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "# Import the deployment classes\n",
    "from example_app.serve.deployments.sentiment import SentimentAnalyzer\n",
    "from example_app.serve.deployments.classification import TextClassifier\n",
    "from example_app.serve.deployments.entities import EntityRecognizer\n",
    "from example_app.serve.deployments.streaming_analyzer import StreamingAnalyzer\n",
    "\n",
    "# Import config\n",
    "from example_app.config import RAY_ADDRESS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Ray and Ray Serve\n",
    "\n",
    "First, we need to initialize Ray and start Ray Serve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Ray (if not already initialized)\n",
    "if not ray.is_initialized():\n",
    "    ray.init(address=RAY_ADDRESS, namespace=\"nlp_pipeline\")\n",
    "    print(\"Ray initialized\")\n",
    "else:\n",
    "    print(\"Ray already initialized\")\n",
    "    \n",
    "# Start Ray Serve\n",
    "serve.start()\n",
    "print(\"Ray Serve started\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Models\n",
    "\n",
    "Now we'll deploy each of the three NLP models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run_many([\n",
    "    serve.RunTarget(\n",
    "        name=\"sentiment_analyzer\",\n",
    "        target=SentimentAnalyzer.options(name=\"sentiment_analyzer\").bind(),\n",
    "        route_prefix=\"/sentiment\"\n",
    "    ),\n",
    "    serve.RunTarget(\n",
    "        name=\"text_classifier\",\n",
    "        target=TextClassifier.options(name=\"text_classifier\").bind(),\n",
    "        route_prefix=\"/text_classifier\"\n",
    "    ),\n",
    "    serve.RunTarget(\n",
    "        name=\"entity_recognizer\",\n",
    "        target=EntityRecognizer.options(name=\"entity_recognizer\").bind(),\n",
    "        route_prefix=\"/entity_recognizer\"\n",
    "    ),\n",
    "    serve.RunTarget(\n",
    "        name=\"streaming_analyzer\",\n",
    "        target=StreamingAnalyzer.options(name=\"streaming_analyzer\").bind(),\n",
    "        route_prefix=\"/streaming_analyzer\"\n",
    "    )\n",
    "])\n",
    "\n",
    "# Wait a moment for deployments to initialize\n",
    "time.sleep(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Deployment Handles\n",
    "\n",
    "Next, we'll get handles to each deployment to make remote calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.serve import get_deployment_handle\n",
    "\n",
    "# Get handles to the deployments\n",
    "sentiment_analyzer = get_deployment_handle(\"sentiment_analyzer\", app_name=\"sentiment_analyzer\")\n",
    "text_classifier = get_deployment_handle(\"text_classifier\", app_name=\"text_classifier\")\n",
    "entity_recognizer = get_deployment_handle(\"entity_recognizer\", app_name=\"entity_recognizer\")\n",
    "streaming_analyzer = get_deployment_handle(\"streaming_analyzer\", app_name=\"streaming_analyzer\").options(stream=True)\n",
    "\n",
    "print(\"Got handles to all deployments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SentimentAnalyzer\n",
    "\n",
    "Let's test the sentiment analyzer with some example sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_sentiment():\n",
    "    sample_texts = [\n",
    "        \"I really enjoyed this movie, it was fantastic!\",\n",
    "        \"The service was terrible and the food was cold.\",\n",
    "        \"This product is okay, not great but not bad either.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Testing Sentiment Analysis ===\\n\")\n",
    "    for text in sample_texts:\n",
    "        result = await sentiment_analyzer.analyze.remote(text)\n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Sentiment: {result['sentiment']} (Score: {result['score']:.4f})\")\n",
    "        print(f\"Processing time: {result['processing_time']:.4f} seconds\\n\")\n",
    "\n",
    "await test_sentiment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test TextClassifier\n",
    "\n",
    "Now let's test the zero-shot text classifier with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_classification():\n",
    "    sample_texts = [\n",
    "        \"The iPhone 13 Pro Max has an amazing camera system and excellent battery life.\",\n",
    "        \"Scientists have discovered a new species of deep-sea fish that can glow in the dark.\",\n",
    "        \"The company reported a 15% increase in quarterly revenue, exceeding analyst expectations.\"\n",
    "    ]\n",
    "    \n",
    "    sample_labels = [\n",
    "        [\"technology\", \"sports\", \"politics\", \"entertainment\"],\n",
    "        [\"science\", \"arts\", \"business\", \"health\"],\n",
    "        [\"finance\", \"technology\", \"politics\", \"environment\"]\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Testing Text Classification ===\\n\")\n",
    "    for i, text in enumerate(sample_texts):\n",
    "        labels = sample_labels[i]\n",
    "        result = await text_classifier.classify.remote(text, labels)\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Labels: {labels}\")\n",
    "        print(\"Results:\")\n",
    "        for label_info in result[\"labels\"]:\n",
    "            print(f\"  - {label_info['label']}: {label_info['score']:.4f}\")\n",
    "        print(f\"Processing time: {result['processing_time']:.4f} seconds\\n\")\n",
    "\n",
    "await test_classification()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test EntityRecognizer\n",
    "\n",
    "Finally, let's test the named entity recognition model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_entity_recognition():\n",
    "    sample_texts = [\n",
    "        \"Apple CEO Tim Cook announced the new iPhone at their headquarters in Cupertino, California yesterday.\",\n",
    "        \"The European Union and the United States are working on new regulations for artificial intelligence.\",\n",
    "        \"Microsoft's CEO Satya Nadella will be speaking at the conference in New York on Friday.\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n=== Testing Entity Recognition ===\\n\")\n",
    "    for text in sample_texts:\n",
    "        result = await entity_recognizer.recognize_entities.remote(text)\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(\"Entities:\")\n",
    "        \n",
    "        # Group entities by type\n",
    "        entities_by_type = {}\n",
    "        for entity in result[\"entities\"]:\n",
    "            entity_type = entity[\"type\"]\n",
    "            if entity_type not in entities_by_type:\n",
    "                entities_by_type[entity_type] = []\n",
    "            entities_by_type[entity_type].append(entity)\n",
    "        \n",
    "        # Print entities by type\n",
    "        for entity_type, entities in entities_by_type.items():\n",
    "            print(f\"  {entity_type}:\")\n",
    "            for entity in entities:\n",
    "                print(f\"    - {entity['text']} (Score: {entity['score']:.4f})\")\n",
    "                \n",
    "        print(f\"Processing time: {result['processing_time']:.4f} seconds\\n\")\n",
    "\n",
    "await test_entity_recognition()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test StreamingAnalyzer\n",
    "\n",
    "Now let's test the streaming analyzer that processes text incrementally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "async def test_streaming_analyzer():\n",
    "    sample_text = \"\"\"The Apple conference in San Francisco was attended by Tim Cook, who announced new products. \n",
    "    Microsoft and Google were also represented at the event. \n",
    "    The new iPhone impressed many analysts with its innovative features. \n",
    "    The event took place at the Moscone Center in downtown San Francisco. \n",
    "    Investors reacted positively to the announcements, with Apple stock rising 3% after the conference.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Testing Streaming Analysis ===\\n\")\n",
    "    print(\"Full text to analyze:\\n\")\n",
    "    print(sample_text)\n",
    "    print(\"\\nStreaming results (processing chunk by chunk):\\n\")\n",
    "    \n",
    "    # Process text in streaming mode\n",
    "    async for chunk_result in streaming_analyzer.stream_analysis.remote(sample_text):\n",
    "        print(f\"\\nChunk {chunk_result['chunk_id'] + 1}/{chunk_result['total_chunks']}:\")\n",
    "        print(f\"Text: {chunk_result['chunk_text']}\")\n",
    "        \n",
    "        if 'sentiment' in chunk_result:\n",
    "            print(f\"Sentiment: {chunk_result['sentiment']} (Score: {chunk_result['sentiment_score']:.4f})\")\n",
    "        \n",
    "        if 'entities' in chunk_result and len(chunk_result['entities']) > 0:\n",
    "            print(\"Entities:\")\n",
    "            for entity in chunk_result['entities']:\n",
    "                print(f\"  - {entity['word']} ({entity['entity']}) - Score: {entity['score']:.4f}\")\n",
    "        \n",
    "        print(f\"Processing time: {chunk_result['processing_time']:.4f} seconds\")\n",
    "        print(f\"Progress: {chunk_result['progress'] * 100:.1f}%\")\n",
    "\n",
    "await test_streaming_analyzer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Test Document Analysis\n",
    "\n",
    "Let's also test the non-streaming document analysis that processes text incrementally but returns a single result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def test_document_analysis():\n",
    "    sample_text = \"\"\"The Apple conference in San Francisco was attended by Tim Cook, who announced new products. \n",
    "    Microsoft and Google were also represented at the event. \n",
    "    The new iPhone impressed many analysts with its innovative features. \n",
    "    The event took place at the Moscone Center in downtown San Francisco. \n",
    "    Investors reacted positively to the announcements, with Apple stock rising 3% after the conference.\"\"\"\n",
    "    \n",
    "    print(\"\\n=== Testing Document Analysis ===\\n\")\n",
    "    print(\"Full text to analyze:\\n\")\n",
    "    print(sample_text)\n",
    "    \n",
    "    # Process the entire document\n",
    "    result = await streaming_analyzer.analyze_document.remote(sample_text)\n",
    "    \n",
    "    print(\"\\nDocument analysis results:\\n\")\n",
    "    print(f\"Text length: {result['text_length']} characters\")\n",
    "    print(f\"Chunk count: {result['chunk_count']} chunks\")\n",
    "    \n",
    "    if 'sentiment' in result:\n",
    "        print(f\"\\nOverall sentiment: {result['sentiment']['overall']}\")\n",
    "        print(f\"Average sentiment score: {result['sentiment']['average_score']:.4f}\")\n",
    "        print(f\"Sentiment by chunk: {result['sentiment']['sentiment_by_chunk']}\")\n",
    "    \n",
    "    if 'entities' in result:\n",
    "        print(f\"\\nTotal entities: {result['entities']['count']}\")\n",
    "        print(\"Entities by type:\")\n",
    "        for entity_type, count in result['entities']['by_type'].items():\n",
    "            print(f\"  - {entity_type}: {count}\")\n",
    "        \n",
    "        print(\"\\nAll entities:\")\n",
    "        for entity in result['entities']['all_entities'][:10]:  # Show first 10 entities only\n",
    "            print(f\"  - {entity['word']} ({entity['entity']}) - Score: {entity['score']:.4f}\")\n",
    "        \n",
    "        if len(result['entities']['all_entities']) > 10:\n",
    "            print(f\"  ... and {len(result['entities']['all_entities']) - 10} more entities\")\n",
    "\n",
    "await test_document_analysis() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Benchmarking\n",
    "\n",
    "Let's run a simple benchmark to measure the throughput of each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def benchmark_models(num_requests=10):\n",
    "    print(\"\\n=== Performance Benchmarking ===\\n\")\n",
    "    \n",
    "    # Test data\n",
    "    sentiment_text = \"This product exceeded my expectations in every way possible!\"\n",
    "    classification_text = \"The president announced new economic policies yesterday.\"\n",
    "    classification_labels = [\"politics\", \"economics\", \"sports\", \"technology\"]\n",
    "    entity_text = \"Google and Microsoft are competing in the cloud services market in Europe.\"\n",
    "    \n",
    "    # Benchmark SentimentAnalyzer\n",
    "    print(f\"Benchmarking SentimentAnalyzer with {num_requests} requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tasks = [sentiment_analyzer.analyze.remote(sentiment_text) for _ in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average time per request: {total_time/num_requests:.4f} seconds\")\n",
    "    print(f\"Throughput: {num_requests/total_time:.2f} requests/second\\n\")\n",
    "    \n",
    "    # Benchmark TextClassifier\n",
    "    print(f\"Benchmarking TextClassifier with {num_requests} requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tasks = [text_classifier.classify.remote(classification_text, classification_labels) for _ in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average time per request: {total_time/num_requests:.4f} seconds\")\n",
    "    print(f\"Throughput: {num_requests/total_time:.2f} requests/second\\n\")\n",
    "    \n",
    "    # Benchmark EntityRecognizer\n",
    "    print(f\"Benchmarking EntityRecognizer with {num_requests} requests...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    tasks = [entity_recognizer.recognize_entities.remote(entity_text) for _ in range(num_requests)]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Total time: {total_time:.4f} seconds\")\n",
    "    print(f\"Average time per request: {total_time/num_requests:.4f} seconds\")\n",
    "    print(f\"Throughput: {num_requests/total_time:.2f} requests/second\\n\")\n",
    "\n",
    "# Run benchmark with 10 requests per model\n",
    "await benchmark_models(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleanup\n",
    "\n",
    "Optionally, we can shut down Ray Serve when we're done testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to shut down Ray Serve\n",
    "# serve.shutdown()\n",
    "# print(\"Ray Serve shut down\")\n",
    "\n",
    "print(\"All tests completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-pipeline-46rPltzW-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
