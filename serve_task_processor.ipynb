{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Task Processing in Ray Serve\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Ray Serve customers need a way to handle long-running API requests asynchronously. For example, when a user requests video or document indexing, the system should enqueue the task in a background queue for later processing. The API should return a quick response while the task executes asynchronously.\n",
    "\n",
    "## Traditional Solution and Its Challenges\n",
    "\n",
    "A common approach is to use a message broker to queue tasks and a stream processor to handle them asynchronously. However, this introduces complexity for customers who lack the infrastructure for online stream processing, requiring them to manage and scale additional components.\n",
    "\n",
    "<img src=\"stream_processor.png\" width=\"800\" />\n",
    "\n",
    "## Proposed Solution\n",
    "\n",
    "For Anyscale customers using Ray Serve, we propose hosting **Celery Workers as Ray Serve Deployments**. Since Celery supports various message brokers (e.g., SQS, Redis, RabbitMQ), users can bring their own message brokers for async task queuing.\n",
    "\n",
    "This approach slightly deviates from Ray Serve’s original request-response model by enabling it to support long-running stream processing applications.\n",
    "\n",
    "<img src=\"serve_stream_processor.png\" width=\"800\" />\n",
    "\n",
    "\n",
    "## Alternatives Considered\n",
    "\n",
    "1. **Using Ray Tasks within a Deployment Handler**  \n",
    "\n",
    "```python\n",
    "indexing_worker.remote(video_id, \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n",
    "```\n",
    "\n",
    "While Ray is designed for distributed task execution, this approach fails because the indexing_worker task would share fate with the MyApp deployment. During service rollouts, Ray's garbage collection may cancel in-progress tasks.\n",
    "\n",
    "2. **Building a Custom Stream Processor**\n",
    "\n",
    "Developing a custom solution to support multiple message brokers would require significant effort. Celery already provides this functionality, making it the better choice.\n",
    "\n",
    "## Current Status\n",
    "Support for Celery within Ray Serve is experimental, and recommendations may evolve. This document serves as an initial exploration of the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "import uuid\n",
    "from celery import Celery\n",
    "from fastapi import FastAPI, Request\n",
    "from ray import serve\n",
    "import ray\n",
    "import redis\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import inspect\n",
    "from ray.serve.handle import DeploymentHandle\n",
    "\n",
    "# dont show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Covered in This Notebook\n",
    "\n",
    "1. **Start Redis Server**  \n",
    "   - Set up a Redis instance to act as the message broker for Celery.\n",
    "\n",
    "2. **Define Two Ray Serve Applications**  \n",
    "   - **`IndexingTasksDeployment`**: Runs Celery workers and contains user-defined tasks. This inherits from serve provided `CeleryWorkerDeployment` class.\n",
    "   - **`MyApp`**: The main Ray Serve application that handles user facing API requests.\n",
    "\n",
    "3. **Deploy and Demonstrate Async Task Processing**  \n",
    "   - Deploy both applications within a single Ray Serve deployment.  \n",
    "   - Show how routes in `MyApp` can enqueue tasks in Celery for asynchronous execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Redis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example we are using redis for both data broker and backend. In real world they can be different and both should be provided by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the serve application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fastapi_app = FastAPI()\n",
    "\n",
    "# Create Celery application\n",
    "celery_app = Celery(\n",
    "    'task_queue', \n",
    "    broker_url='redis://localhost',\n",
    "    backend='redis://localhost'\n",
    ")\n",
    "celery_app.control.purge()\n",
    "# prefork is not supported in serve deployment actor because we want the worker (and tasks)\n",
    "# to be in the same process as the replica actor.\n",
    "# this means that cpu intensive tasks will block the main thread\n",
    "configs = {\n",
    "    'worker_pool': 'threads',\n",
    "    'worker_concurrency': 10,\n",
    "}\n",
    "celery_app.conf.update(configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code in the following cell should go into ray serve as a python module for users to inherit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_handler(func):\n",
    "    func.is_task_handler = True  # Mark the function\n",
    "    return func\n",
    "\n",
    "class CeleryWorkerDeployment:\n",
    "    \"\"\"\n",
    "    This is a base class that users will inherit from to create their own celery worker deployments.\n",
    "    \"\"\"\n",
    "    def __init__(self, celery_app: Celery):\n",
    "        self.celery_app = celery_app\n",
    "        assert self.celery_app.conf.get('worker_pool') == 'threads', \"Only threads pool is supported in serve deployment actor\"\n",
    "        self.celery_worker = celery_app.Worker()\n",
    "        def run_worker():\n",
    "            # starting celery worker here so that its part of the same process as the replica actor\n",
    "            self.celery_worker.start()\n",
    "        \n",
    "        # Find and register all methods marked with @task_handler\n",
    "        for name, method in inspect.getmembers(self, predicate=inspect.ismethod):\n",
    "            if getattr(method, \"is_task_handler\", False):\n",
    "                celery_app.task(method, name=name)\n",
    "                print(f\"Registered task: {name}\")\n",
    "        \n",
    "        self.celery_worker_thread = threading.Thread(target=run_worker)\n",
    "        self.celery_worker_thread.start()\n",
    "    \n",
    "    def __del__(self):\n",
    "        # allow the worker to finish all the tasks that are being processed.\n",
    "        # This does not guarantee that all tasks in the queue are finished.\n",
    "        print(\"stopping worker, will wait for all tasks to finish\")\n",
    "        self.celery_worker.stop()\n",
    "        self.celery_worker_thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    name=\"indexing_tasks_deployment\",\n",
    "    num_replicas=5,\n",
    ")\n",
    "class IndexingTasksDeployment(CeleryWorkerDeployment):\n",
    "    \"\"\"\n",
    "    This is application code that users will write.\n",
    "    \"\"\"\n",
    "    def __init__(self, celery_app: Celery):\n",
    "        super().__init__(celery_app)\n",
    "\n",
    "        # create a large numpy array\n",
    "        self.arr = np.random.rand(1000000)\n",
    "        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "    @task_handler\n",
    "    def document_indexing_task(self, document_id: str):\n",
    "        \"\"\"\n",
    "        blocking IO operations are allowed here since they are running in a thread from ThreadPoolExecutor\n",
    "        \"\"\"\n",
    "\n",
    "        # set the status to pending\n",
    "        job = json.loads(self.redis_client.get(document_id))\n",
    "        job[\"status\"] = \"pending\"\n",
    "        self.redis_client.set(document_id, json.dumps(job))\n",
    "\n",
    "        # task has access to replica actor object because tasks are running in the same process\n",
    "        # so there is no serialization overhead\n",
    "        s = np.sum(self.arr)\n",
    "\n",
    "        # example of making a http request\n",
    "        # requests.get(f\"http://localhost:8000/my_app/index_document_sync?document_id={document_id}\").json()\n",
    "\n",
    "        # example of calling another deployment\n",
    "        # call the deployment\n",
    "        return serve.get_app_handle(\"my_app\").index_document.remote(document_id).result()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(\n",
    "    name=\"my_app\",\n",
    "    # since celery tasks are calling into `index_document_sync`, set this parameter appropriately.\n",
    "    max_ongoing_requests=100\n",
    ")\n",
    "@serve.ingress(fastapi_app)\n",
    "class MyApp:\n",
    "    \"\"\"\n",
    "    This is application code that users will write.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "    @fastapi_app.get(\"/index_document_async\")\n",
    "    def index_document_async(self, document_id: str):\n",
    "\n",
    "        # async call to celery to enqueue the task\n",
    "        # task name is the name of the function decorated with @task_handler in the celery worker deployment\n",
    "        celery_result = celery_app.send_task(name=\"document_indexing_task\", args=(document_id,))\n",
    "\n",
    "        \"\"\"\n",
    "        In order to track the status of the task, the user may choose to make an entry into their own database\n",
    "        about the request. The idea is that the caller will poll the status of the task from their own database.\n",
    "\n",
    "        Serve does not provide any built-in support for task status tracking because needs of different users will vary.\n",
    "        \"\"\"\n",
    "        response = {\"status\": \"not_started\", \"start_time\": time.time(), \"celery_task_id\": celery_result.id}\n",
    "        self.redis_client.set(document_id, json.dumps(response))\n",
    "        return response\n",
    "\n",
    "    @fastapi_app.get(\"/index_document_sync\")\n",
    "    async def index_document(self, document_id: str):\n",
    "        \"\"\"\n",
    "        This is a heavyweight operation that will be run as part of task processing.\n",
    "        \"\"\"\n",
    "        await asyncio.sleep(5)\n",
    "        # update the status to done and timestamp and calculate the time taken\n",
    "        data = json.loads(self.redis_client.get(document_id))\n",
    "        start_time = data[\"start_time\"]\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "\n",
    "        data[\"status\"] = \"done\"\n",
    "        data[\"time_taken\"] = time_taken\n",
    "        data[\"end_time\"] = end_time\n",
    "\n",
    "        self.redis_client.set(document_id, json.dumps(data))\n",
    "        return f\"Document {document_id} indexed in {time_taken} seconds\"\n",
    "    \n",
    "    @fastapi_app.get(\"/index_document_status\")\n",
    "    def index_document_status(self, document_id: str):\n",
    "        \"\"\"\n",
    "        Status tracking is left to the user. This is just an example of how to track the status of the task.\n",
    "        This is just an example of how to track the status of the task, your mileage may vary.\n",
    "        \"\"\"\n",
    "\n",
    "        # get the status from the database, this status is tracked by user. This is probably the most common way users will track the status of the task\n",
    "        db_status = json.loads(self.redis_client.get(document_id))\n",
    "\n",
    "        # but if the user wants to track the status of the task in celery, they can do so by calling this endpoint\n",
    "        # this code exists for demonstration purposes\n",
    "        celery_result_async = celery_app.AsyncResult(db_status[\"celery_task_id\"])\n",
    "        if celery_result_async.successful():\n",
    "            # if the task is still running, calling get will block until the task is finished\n",
    "            celery_result = celery_result_async.get()\n",
    "        else:\n",
    "            celery_result = None\n",
    "\n",
    "        # return the status\n",
    "        return {\n",
    "            \"db_status\": db_status[\"status\"],\n",
    "            \"celery_status\": celery_result_async.status,\n",
    "            \"celery_result\": celery_result\n",
    "        }\n",
    "\n",
    "task_processor = IndexingTasksDeployment.bind(celery_app)\n",
    "app = MyApp.bind()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(app, name=\"my_app\", route_prefix=\"/my_app\")\n",
    "serve.run(task_processor, name=\"task_processor\", route_prefix=\"/task_processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make async Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of 30 document ids\n",
    "num_tasks = 300\n",
    "document_ids = [str(uuid.uuid4()) for _ in range(num_tasks)]\n",
    "# make 30 parallel requests to the task processor\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "def make_request(document_id):\n",
    "    requests.get(f\"http://localhost:8000/my_app/index_document_async?document_id={document_id}\")    \n",
    "\n",
    "threads = []\n",
    "for document_id in document_ids:\n",
    "    thread = threading.Thread(target=make_request, args=(document_id,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poll for task status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_tasks_done(document_ids):\n",
    "    for document_id in document_ids:\n",
    "        response = requests.get(f\"http://localhost:8000/my_app/index_document_status?document_id={document_id}\").json()\n",
    "        if response[\"db_status\"] != \"done\":\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# wait until all tasks are done\n",
    "while not are_tasks_done(document_ids):\n",
    "    time.sleep(1)\n",
    "\n",
    "# plot the time taken for each task add\n",
    "time_taken = [json.loads(redis_client.get(document_id))[\"time_taken\"] for document_id in document_ids]\n",
    "plt.hist(time_taken, bins=10)\n",
    "plt.xlabel('Time Taken (seconds)')\n",
    "plt.ylabel('Number of Tasks')\n",
    "plt.title('Distribution of Task Processing Times')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why did it take **30 seconds** for all tasks to complete?\n",
    "\n",
    "1. We queued **300 tasks**.  \n",
    "2. There are **3 Celery workers**, each running with a concurrency of **10** (i.e., **3 × 10 = 30** tasks can run in parallel).  \n",
    "3. Each task takes **5 seconds** to complete (`time.sleep(5)`).  \n",
    "4. Since **30 tasks run concurrently**, the total execution time is:  \n",
    "   $$\n",
    "   \\frac{300 \\text{ tasks}}{30 \\text{ concurrent workers}} \\times 5 \\text{ seconds} = 30 \\text{ seconds}\n",
    "   $$\n",
    "5. As expected, all tasks complete in **30 seconds**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get stutus\n",
    "requests.get(f\"http://localhost:8000/my_app/index_document_status?document_id={document_ids[-1]}\").json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get task status from redis for all tasks count status done\n",
    "count = 0\n",
    "pending = 0\n",
    "not_started = 0\n",
    "for document_id in document_ids:\n",
    "    job = json.loads(redis_client.get(document_id))\n",
    "    if job[\"status\"] == \"done\":\n",
    "        count += 1\n",
    "    elif job[\"status\"] == \"pending\":\n",
    "        pending += 1\n",
    "    else:\n",
    "        not_started += 1\n",
    "    \n",
    "print(f\"count of done tasks: {count}, pending tasks: {pending}, not started tasks: {not_started}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the histogram, all tasks finish around 5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats, Considerations, and Recommendations\n",
    "\n",
    "1. **Task Delivery Guarantees Depend on the Message Broker**  \n",
    "   - Different message brokers offer varying guarantees. For example, **SQS provides strong delivery guarantees**, whereas **Redis does not**.  \n",
    "   - Refer to the [Celery documentation](https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/) for broker and backend choices.\n",
    "\n",
    "2. **At-Least-Once Processing Is Configuration Dependent**  \n",
    "   - At-least-once processing in Celery depends on configuration parameters like:  \n",
    "     - `task_acks_late`  \n",
    "     - `task_acks_on_failure_or_timeout`  \n",
    "     - `task_publish_retry`  \n",
    "     - `task_publish_retry_policy`  \n",
    "   - By default, Celery **does not guarantee at-least-once processing**. See [Celery FAQ](https://docs.celeryq.dev/en/stable/faq.html#faq-acks-late-vs-retry) for details.\n",
    "\n",
    "3. **TaskProcessor Deployment Should Only Contain Celery Tasks** *(Recommendation, Not a Strict Requirement)*  \n",
    "   - This simplifies future autoscaling, as task processor scaling can be based purely on queue characteristics.  \n",
    "   - Mixing API routes in `TaskProcessor` could complicate autoscaling.\n",
    "\n",
    "4. **Avoid Including TaskProcessor in Deployment Chains**  \n",
    "   - There is **no need** to include `TaskProcessor` in any deployment chain since Celery tasks are invoked using:  \n",
    "     ```python\n",
    "     celery_app.send_task(\"document_indexing_task\", args=(document_id,))\n",
    "     ```  \n",
    "   - This remains true until we find a way to support:  \n",
    "     ```python\n",
    "     document_indexing_task.delay(document_id)\n",
    "     ```\n",
    "\n",
    "5. **Celery Workers Run Inside Ray Actors** *(Key Design Choice)*  \n",
    "   - Running Celery workers **inside Ray actors** ensures Ray manages their lifecycle, preventing zombie processes if an actor dies.  \n",
    "   - However, this limits the Celery execution pool to **threads**. See [Celery Worker Pools](https://celery.school/celery-worker-pools) for available options.\n",
    "\n",
    "6. **Concurrency Considerations**  \n",
    "   - **For I/O-intensive tasks:** Increase Celery worker concurrency.  \n",
    "   - **For CPU-intensive tasks:** Increasing thread concurrency **won’t help** due to the **GIL**. Instead, increase `num_replicas` on `TaskProcessor`.\n",
    "\n",
    "7. **Celery Tasks That Call Other Deployments May Be Bottlenecked**  \n",
    "   - If a Celery task invokes another Ray Serve deployment, **its concurrency is limited** by the `max_ongoing_requests` of the target deployment.\n",
    "\n",
    "8. **Status Tracking of Tasks**\n",
    "\n",
    "When tracking the status of asynchronous tasks queued in Celery, users have two options:  \n",
    "- **Using Celery Task ID:**  \n",
    "   Obtain the Celery task ID from `celery_app.send_task` and use `celery_app.AsyncResult(celery_task_id)` to poll for task status.  \n",
    "- **Custom Status Tracking:**  \n",
    "   Generate a unique task ID, store it in a database, and update the task status in the database when execution completes.  \n",
    "\n",
    "The choice between these approaches depends on the application's requirements. Since this falls outside the immediate scope of Ray Serve, we currently do not provide built-in status tracking. However, future support may be considered.\n",
    "\n",
    "9. **Backward and Forward Compatibility of Tasks During Application Rollouts**\n",
    "\n",
    "During an **up-then-down** deployment, where two versions of a task processor run concurrently, compatibility issues may arise.  \n",
    "\n",
    "**Best Practices for Version Rollouts**  \n",
    "- **Drain Pending Tasks Before Upgrading**  \n",
    "   Before starting Celery workers with a new application version, ensure that all tasks enqueued by the old version have been processed. If tasks remain in the queue and the new version modifies the task definition incompatibly, they may either:  \n",
    "     - Remain unprocessed in the queue.  \n",
    "     - Be incorrectly routed to an unintended task.\n",
    "   Ray serve does not guard against this, recommendation to user is not make such drastic change to the app. The only thing serve ensures is that, when a deployment actor is shutting down, it will do best effort to stop accepting new tasks and all ongoing tasks are completed before it shuts down.\n",
    "\n",
    "- **Use Stable Data Serialization**  \n",
    "   To safeguard against backward and forward compatibility issues, use structured encoding formats like **Protocol Buffers (protobuf)** or **Avro** for task parameters and return values.  \n",
    "\n",
    "**Task Persistence Considerations**  \n",
    "The duration a task remains in the queue depends on the **message broker configuration** and whether an `expires` value was set when the task was enqueued. By default, if no workers are available, tasks can sit in the queue indefinitely. This is particularly important when an application rollout **removes a task handler**, as unprocessed tasks may be left in an orphaned state.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Future Work\n",
    "\n",
    "Currently, this solution is **DIY**, but future improvements should aim for deeper integration:\n",
    "\n",
    "1. **Move `TaskProcessor` Into Ray Serve**  \n",
    "   - Reduce boilerplate by abstracting Celery setup from users.\n",
    "\n",
    "2. **Enable Autoscaling for Celery Workers**  \n",
    "   - Implement autoscaling based on queue depth and processing load.\n",
    "\n",
    "3. **Harden the system for failiure conditions**\n",
    "   - when app rollout, do we loose messages in queue?\n",
    "   - what happens during replica fail over?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()\n",
    "ray.shutdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
