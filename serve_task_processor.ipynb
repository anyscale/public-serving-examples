{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Task Processing in Ray Serve\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "Ray Serve customers need a way to handle long-running API requests asynchronously. For example, when a user requests video or document indexing, the system should enqueue the task in a background queue for later processing. The API should return a quick response while the task executes asynchronously.\n",
    "\n",
    "## Traditional Solution and Its Challenges\n",
    "\n",
    "A common approach is to use a message broker to queue tasks and a stream processor to handle them asynchronously. However, this introduces complexity for customers who lack the infrastructure for online stream processing, requiring them to manage and scale additional components.\n",
    "\n",
    "![Task Processing Architecture](./stream_processor.png)\n",
    "\n",
    "## Proposed Solution\n",
    "\n",
    "For Anyscale customers using Ray Serve, we propose hosting **Celery Workers as Ray Serve Deployments**. Since Celery supports various message brokers (e.g., SQS, Redis, RabbitMQ), users can bring their own message brokers for async task queuing.\n",
    "\n",
    "This approach slightly deviates from Ray Serve’s original request-response model by enabling it to support long-running stream processing applications.\n",
    "\n",
    "![Task Processing Architecture](./serve_stream_processor.png)\n",
    "\n",
    "## Alternatives Considered\n",
    "\n",
    "1. **Using Ray Tasks within a Deployment Handler**  \n",
    "\n",
    "```python\n",
    "indexing_worker.remote(video_id, \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\")\n",
    "```\n",
    "\n",
    "While Ray is designed for distributed task execution, this approach fails because the indexing_worker task would share fate with the MyApp deployment. During service rollouts, Ray's garbage collection may cancel in-progress tasks.\n",
    "\n",
    "2. **Building a Custom Stream Processor**\n",
    "\n",
    "Developing a custom solution to support multiple message brokers would require significant effort. Celery already provides this functionality, making it the better choice.\n",
    "\n",
    "## Current Status\n",
    "Support for Celery within Ray Serve is experimental, and recommendations may evolve. This document serves as an initial exploration of the approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import threading\n",
    "import time\n",
    "import uuid\n",
    "from celery import Celery\n",
    "from fastapi import FastAPI, Request\n",
    "from ray import serve\n",
    "import ray\n",
    "import redis\n",
    "import requests\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# dont show warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What's Covered in This Notebook\n",
    "\n",
    "1. **Start Redis Server**  \n",
    "   - Set up a Redis instance to act as the message broker for Celery.\n",
    "\n",
    "2. **Define Two Ray Serve Applications**  \n",
    "   - **`TaskProcessor`**: Runs Celery workers and contains user-defined tasks.  \n",
    "   - **`MyApp`**: The main Ray Serve application that handles API requests.\n",
    "\n",
    "3. **Deploy and Demonstrate Async Task Processing**  \n",
    "   - Deploy both applications within a single Ray Serve deployment.  \n",
    "   - Show how routes in `MyApp` can enqueue tasks in Celery for asynchronous execution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Redis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker run -d --name redis-stack-server -p 6379:6379 redis/redis-stack-server:latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the serve application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init(ignore_reinit_error=True)\n",
    "\n",
    "fastapi_app = FastAPI()\n",
    "\n",
    "# Create Celery application\n",
    "celery_app = Celery('task_queue', \n",
    "                    broker_url='redis://localhost',\n",
    "                    backend='redis://localhost')\n",
    "\n",
    "@serve.deployment(\n",
    "    name=\"task_processor\",\n",
    "    num_replicas=10,\n",
    ")\n",
    "class TaskProcessor:\n",
    "    \"\"\"\n",
    "    A single deployment that is responsible for all task processing\n",
    "    across all serve applications.\n",
    "\n",
    "    Its recommended that this deployment is not passed to other deployments\n",
    "    as a handler because\n",
    "    1. the way we would enqueue task is by calling celery_app.send_task(\"document_indexing_task\", args=(document_id,))\n",
    "    so there is no reason to use the handler to call into this deployment.\n",
    "    2. \n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.worker = celery_app.Worker(\n",
    "            # prefork is not supported in serve deployment actor because we want the worker (and tasks)\n",
    "            # to be in the same process as the replica actor.\n",
    "            # this means that cpu intensive tasks will block the main thread\n",
    "            pool_cls='threads',\n",
    "            loglevel='INFO',\n",
    "            concurrency=10\n",
    "        )\n",
    "        def run_worker():\n",
    "            \"\"\"\n",
    "            starting worker here so that its part of the same process as the replica actor\n",
    "\n",
    "            we want to use threads here because the tasks that worker\n",
    "            spawns should be the same process as the worker and replica actor.\n",
    "            the other option is to use solo, but that does not provide any\n",
    "            concurrency.\n",
    "            \"\"\"\n",
    "            \n",
    "            self.worker.start()\n",
    "        \n",
    "        # TODO: automatically register all tasks in the celery app, this enumeration is error prone\n",
    "        celery_app.task(self.document_indexing_task, name=\"document_indexing_task\", acks_late=True)\n",
    "        \n",
    "        worker_thread = threading.Thread(target=run_worker)\n",
    "        worker_thread.start()\n",
    "        \n",
    "        # create a large numpy array\n",
    "        self.arr = np.random.rand(1000000)\n",
    "    \n",
    "    def __del__(self):\n",
    "        # allow the worker to finish all the tasks in the queue\n",
    "        self.worker.stop()\n",
    "    \n",
    "    def document_indexing_task(self, document_id: str):\n",
    "        \"\"\"\n",
    "        blocking IO operations are allowed here since they are running in a thread from ThreadPoolExecutor\n",
    "        \"\"\"\n",
    "\n",
    "        # task has access to replica actor object because tasks are running in the same process\n",
    "        # so there is no serialization overhead\n",
    "        print(np.sum(self.arr))\n",
    "\n",
    "        # example of making a http request\n",
    "        # requests.get(f\"http://localhost:8000/my_app/index_document_sync?document_id={document_id}\").json()\n",
    "\n",
    "        # example of calling another deployment\n",
    "        serve.get_app_handle(\"my_app\").index_document_sync.remote(document_id).result()\n",
    "        return\n",
    "\n",
    "\n",
    "@serve.deployment(\n",
    "    name=\"my_app\",\n",
    "    # since celery tasks are calling into `index_document_sync`, set this parameter appropriately.\n",
    "    max_ongoing_requests=100\n",
    ")\n",
    "@serve.ingress(fastapi_app)\n",
    "class MyApp:\n",
    "    def __init__(self):\n",
    "        self.redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "    @fastapi_app.get(\"/index_document_async\")\n",
    "    def index_document_async(self, request: Request):\n",
    "        # write to redis\n",
    "        document_id = request.query_params.get(\"document_id\")\n",
    "        # set the status to pending and timestamp\n",
    "        self.redis_client.set(document_id, json.dumps({\"status\": \"pending\", \"start_time\": time.time()}))\n",
    "        celery_app.send_task(\"document_indexing_task\", args=(document_id,))\n",
    "        return\n",
    "    \n",
    "    @fastapi_app.get(\"/index_document_sync\")\n",
    "    async def index_document_sync(self, request: Request):\n",
    "        document_id = request\n",
    "        await asyncio.sleep(5)\n",
    "        # update the status to done and timestamp and calculate the time taken\n",
    "        data = json.loads(self.redis_client.get(document_id))\n",
    "        start_time = data[\"start_time\"]\n",
    "        end_time = time.time()\n",
    "        time_taken = end_time - start_time\n",
    "        self.redis_client.set(document_id, json.dumps({\"status\": \"done\", \"start_time\": start_time, \"time_taken\": time_taken, \"end_time\": end_time}))\n",
    "        return\n",
    "    \n",
    "\n",
    "app = MyApp.bind()\n",
    "task_processor = TaskProcessor.bind()\n",
    "\n",
    "\"\"\"\n",
    "its recommended to run task processor in a separate serve application\n",
    "the reason is that if the task processor is part of the same serve application, then its difficult to\n",
    "get the deployment topology right and that can get unmanageable.\n",
    "\"\"\"\n",
    "serve.run(app, name=\"my_app\", route_prefix=\"/my_app\")\n",
    "serve.run(task_processor, name=\"task_processor\", route_prefix=\"/task_processor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make async Requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of 30 document ids\n",
    "document_ids = [str(uuid.uuid4()) for _ in range(30)]\n",
    "# make 30 parallel requests to the task processor\n",
    "import requests\n",
    "import threading\n",
    "\n",
    "def make_request(document_id):\n",
    "    requests.get(f\"http://localhost:8000/my_app/index_document_async?document_id={document_id}\")    \n",
    "\n",
    "threads = []\n",
    "for document_id in document_ids:\n",
    "    thread = threading.Thread(target=make_request, args=(document_id,))\n",
    "    thread.start()\n",
    "    threads.append(thread)\n",
    "\n",
    "for thread in threads:\n",
    "    thread.join()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poll for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def are_tasks_done(document_ids):\n",
    "    return all([json.loads(redis_client.get(document_id))[\"status\"] == \"done\" for document_id in document_ids]) \n",
    "\n",
    "# wait until all tasks are done\n",
    "while not are_tasks_done(document_ids):\n",
    "    time.sleep(1)\n",
    "\n",
    "# plot the time taken for each task add\n",
    "time_taken = [json.loads(redis_client.get(document_id))[\"time_taken\"] for document_id in document_ids]\n",
    "plt.hist(time_taken, bins=10)\n",
    "plt.xlabel('Time Taken (seconds)')\n",
    "plt.ylabel('Number of Tasks')\n",
    "plt.title('Distribution of Task Processing Times')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the histogram, all tasks finish around 5 seconds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caveats, Considerations, and Recommendations\n",
    "\n",
    "1. **Task Delivery Guarantees Depend on the Message Broker**  \n",
    "   - Different message brokers offer varying guarantees. For example, **SQS provides strong delivery guarantees**, whereas **Redis does not**.  \n",
    "   - Refer to the [Celery documentation](https://docs.celeryq.dev/en/stable/getting-started/backends-and-brokers/) for broker and backend choices.\n",
    "\n",
    "2. **At-Least-Once Processing Is Configuration Dependent**  \n",
    "   - At-least-once processing in Celery depends on configuration parameters like:  \n",
    "     - `task_acks_late`  \n",
    "     - `task_acks_on_failure_or_timeout`  \n",
    "     - `task_publish_retry`  \n",
    "     - `task_publish_retry_policy`  \n",
    "   - By default, Celery **does not guarantee at-least-once processing**. See [Celery FAQ](https://docs.celeryq.dev/en/stable/faq.html#faq-acks-late-vs-retry) for details.\n",
    "\n",
    "3. **TaskProcessor Deployment Should Only Contain Celery Tasks** *(Recommendation, Not a Strict Requirement)*  \n",
    "   - This simplifies future autoscaling, as task processor scaling can be based purely on queue characteristics.  \n",
    "   - Mixing API routes in `TaskProcessor` could complicate autoscaling.\n",
    "\n",
    "4. **Avoid Including TaskProcessor in Deployment Chains**  \n",
    "   - There is **no need** to include `TaskProcessor` in any deployment chain since Celery tasks are invoked using:  \n",
    "     ```python\n",
    "     celery_app.send_task(\"document_indexing_task\", args=(document_id,))\n",
    "     ```  \n",
    "   - This remains true until we find a way to support:  \n",
    "     ```python\n",
    "     document_indexing_task.delay(document_id)\n",
    "     ```\n",
    "\n",
    "5. **Celery Workers Run Inside Ray Actors** *(Key Design Choice)*  \n",
    "   - Running Celery workers **inside Ray actors** ensures Ray manages their lifecycle, preventing zombie processes if an actor dies.  \n",
    "   - However, this limits the Celery execution pool to **threads**. See [Celery Worker Pools](https://celery.school/celery-worker-pools) for available options.\n",
    "\n",
    "6. **Concurrency Considerations**  \n",
    "   - **For I/O-intensive tasks:** Increase Celery worker concurrency.  \n",
    "   - **For CPU-intensive tasks:** Increasing thread concurrency **won’t help** due to the **GIL**. Instead, increase `num_replicas` on `TaskProcessor`.\n",
    "\n",
    "7. **Celery Tasks That Call Other Deployments May Be Bottlenecked**  \n",
    "   - If a Celery task invokes another Ray Serve deployment, **its concurrency is limited** by the `max_ongoing_requests` of the target deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## Future Work\n",
    "\n",
    "Currently, this solution is **DIY**, but future improvements should aim for deeper integration:\n",
    "\n",
    "1. **Move `TaskProcessor` Into Ray Serve**  \n",
    "   - Reduce boilerplate by abstracting Celery setup from users.\n",
    "\n",
    "2. **Enable Autoscaling for Celery Workers**  \n",
    "   - Implement autoscaling based on queue depth and processing load.\n",
    "\n",
    "3. **Automate Celery Task Registration**  \n",
    "   - Remove the need for users to manually register Celery tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
